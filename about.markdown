---
layout: home
title: ""
list_title: 'News'
permalink: /home/
redirect_from: 
    - /
    - /about/
---
<h1 style="text-align: center;">About</h1>

<!-- - I am a permanent researcher in Inria Saclay in TAU team. My research interests are wide and include but not exclusively reinforcement learning, AI4Science, efficient training of neural networks.
- Previously, I did a PostDoc in Scool team. I was working on how to use Physics-Informed Neural Networks in solving Hamilton-Jacobi-Bellman equations that appear in Continuous-Time Reinforcement Learning problems. I was also working on Entropy Regularised Reinforcement Learning and how to compare reinforcement learning algorithms.
- Before, I did my PhD in Computer Science in University of Bordeaux and Inria Bordeaux. During my PhD, I was working on the topics at the convergence between High Performance Computing (HPC) and Artificial Intelligence (AI). My main objective was to study how efficiently to train deep neural networks in terms of memory usage. My thesis title is [Memory Saving Strategies for Deep Neural Network Training](https://www.theses.fr/2021BORD0335) supervised by Olivier Beaumont and Alexis Joly. -->

- Currently, I am a permanent researcher at Inria Saclay within the TAU team, where my research interests encompass a broad spectrum, with a particular focus on reinforcement learning, AI4Science, 
and the efficient training of neural networks.
- During my previous postdoctoral position in the Scool team, I explored the application of Physics-Informed Neural Networks to tackle Hamilton-Jacobi-Bellman equations in Continuous-Time 
Reinforcement Learning problems. I also delved into Entropy Regularised Reinforcement Learning and developed methods for comparing reinforcement learning algorithms.
- Before, I completed my PhD in Computer Science in University of Bordeaux and Inria Bordeaux. During my PhD, I was working on the topics at the convergence between High Performance Computing (HPC) and Artificial Intelligence (AI). My primary objective during this period was to investigate strategies for memory efficient training of deep neural networks. My thesis, titled ['Memory Saving Strategies for Deep Neural Network Training'](https://www.theses.fr/2021BORD0335), was supervised by Dr. Olivier Beaumont and Dr. Alexis Joly.
<!-- - Research interests: Efficient Training of Neural Networks, Reinforcement Learning, Deep Learning, AI for Science -->

Check out my [CV]({{site.url}}/assets/CV_AleShilova.pdf) if you want to learn more.
